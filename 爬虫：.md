### annode中jupyter快捷键：

在当前块前新增： a

在当前块后增加：b

删除cell ： x

执行cell ： shift+enter

切换cell的模式： m，y

自动补全：tab

打开帮助文档：shift+tab 



### 爬虫：

#### 什么是爬虫：

​	通过编写程序模拟浏览器发请求，让其去互联网中爬取数据

#### 分类：

​	通用爬虫：抓取一整张页面源码的数据

​	聚焦爬虫：抓取一整张页面中某一部分的数据，聚焦爬虫是在通用爬虫基础上实现

​	增量式：检测网站数据更新的情况，一遍将最新更新出来的数据进行爬取、

#### 合法性：

1. 干扰了原网站的正常性运营
2. 爬取到了收法律保护的特定类型的数据或信息
3. 作为一个爬虫从业者一定要谨记：严格遵守网站设置的robots协议
4. 在规避反爬虫的同时也要注意优化自己的代码，不能影响网站的正常运营
5. 在抓取数据的过程中注意避免用户个人信息的泄露

#### 流程：

1. 指定url
2. 发起请求
3. 获取响应数据
4. 持久化存储

#### requests模块：

安装：pip install requests

```python
简单爬取搜狗主页并破解乱码问题：

url = '搜索殷勤网址'
keywords=input('请输入一个关键字:')
params ={
    'query':keywords
}
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'
}

response = requests.get(url=url,params=params,headers=headers)
response.encoding='utf-8'
data = response.text
file = keywords + '.html'
with open(file,'w',encoding='utf-8')as f:
    f.write(data)
    print('下载完成')
```

问题：乱码问题主要是由于我们发送的请求被浏览器识别成为一个异常请求（异常请求：指不是由浏览器发起的请求都被认为是异常请求）

##### 反爬机制：

UA检测,携带headers请求头

##### 反反爬策略：

UA伪装

##### 抓包：用来检测数据是不是动态加载的

动态加载：是指通过另一个请求单独请求到的数据

抓包工具就是捕获到所有的数据包，然后找到浏览器的地址栏url对应的数据包response这个选项卡中进行局部搜索（搜到/搜不到）

###### 搜到：

数据不是动态加载的

解决办法：直接对浏览器地址栏的url地址发起请求获取数据

###### 搜不到：

数据是动态加载的，

解决办法：基于抓包工具进行全局搜索数据

##### 数据解析：

###### 作用：实现聚焦爬虫

###### 实现的方式：

1. 正则
2. bs4：重点
3. xpath：重点
4. pyquery：自学了解

###### 通用原理：

解析的一定是html页面中的源码数据

1. 标签中存储的文本内容
2. 标签属性的属性值

原理：

1. 标签定位
2. 取文本或者取属性

###### 流程：

1. 指定url
2. 发请求
3. 获取响应数据
4. 获取解析
5. 持久化存储

```
	* : 任意多次  >=0
    + : 至少1次   >=1
    ? : 可有可无  0次或者1次
    {m} ：固定m次 hello{3,}
    {m,} ：至少m次
    {m,n} ：m-n次
边界：
    $ : 以某某结尾 
    ^ : 以某某开头
分组：
    (ab)  
	贪婪模式： .*
	非贪婪（惰性）模式： .*?

re.I : 忽略大小写
re.M ：多行匹配
re.S ：单行匹配

re.sub(正则表达式, 替换内容, 字符串)
```

##### bs4解析：

###### 环境安装：

1. pip install bs4
2. pip install lxml

###### 解析原理：

1. 实例化一个BeautifulSoup的对象，需要将等待被解析的页面源码数据加载到该对象中
2. 需要调用BeautifulSoup对象中相关的属性和方法进行标签定位和为本数据的提取

BeautifulSoup如何实例化：

1. BeautifulSoup（fp,'lxml'），将本地存储的一张html文件中的指定数据进行解析
2. BeautifulSoup（page_text,'lxml'），将从互联网中爬取到的数据直接进行数据解析

###### 标签定位：

1. soup.tagName:定位到第一个出现的tagName标签

2. 属性定位：根据指定的属性进行对应标签的定位

   1. **soup.find('tagName',attrName='value')**
   2. **soup.find_all('tagName',attrName='value')**

3. 选择器定位：

   1. soup.select(‘选择器’)

   2. 层器选择器：

      1. 大于号：表示一个层级
      2. 空格：表示多个层级

   3. 取文本：

      1. tag.string:取出直系的文本内容
      2. tag.text 取出所有的文本内容

   4. 取属性

      ​	tag['attrName']

##### xpath解析：

###### 环境安装：

pip install lxml

###### 解析原理：

1. 实例化一个etree的对象，且将被解析的页面源码数据加载到该对象中
2. 使用etree对象中的xpath方法结合着不同形式的xpath表达式进行标签定位和数据的提取

###### 实例化对象：

1. etree.parse('filePath'):将本地存储的一个html文件中的数据加载到实例化好的etree对象中
2. etree.HTML(page_text):将网页在线的html文件加载到实例化好的etree对象中

###### xpath表达式：

1. 标签定位：
   1. 最左侧的/:必须从根节点开始定位标签（几乎不用）
   2. 非最左侧的/：表示一个层级
   3. 最左侧的//：可以从任意位置进行指定标签的定位（最常用）
   4. 非最左侧的//:表示多个层级
   5. 属性定位：//tagName[@attrName="value"]
   6. 索引定位：//tagNamne[index]，索引是从1开始
   7. //div[contains(@class, "ng")]
   8. //div[starts-with(@class, "ta")]
2. 文本定位：
   1. /text():取出直系的文本内容
   2. //text()：取出的是所有的文本
3. 属性定位：
   1. /@attrName

##### 代理（反爬机制）：proxies：｛'key':'value｝

###### 概念：代理服务器

###### 代理服务器的作用

1. 拦截请求和响应，进行转发

###### 代理和爬虫之间的关联是什么

如果pc端的ip被禁掉以后，我们就可以使用代理的机制替换ip进行在访问

###### 如何获取代理的IP地址：

1. 快代理
2. 西祠代理
3. goubanjia
4. 代理精灵：推荐使用

###### 匿名度：

1. 透明：对方知道你使用了代理，也知道你的真实ip
2. 匿名；对方知道你使用了代理，但不知道你的真实ip
3. 高匿名：对方不知道你使用了代理，也不知道你的真实ip

###### 类型：

1. http:只可以拦截转发http协议的请求
2. https：只可以转发拦截https的请求

```python
参数中携带代理ip进行访问
import requests
from lxml import etree
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'
}
url = 'https://www.sogou.com/web?query=ip&_asf=www.sogou.com&_ast=&w=01019900&p=40040100&ie=utf8&from=index-nologin&s_from=index&sut=623&sst0=1578274254338&lkt=2%2C1578274253715%2C1578274253785&sugsuv=000AE8ACDDDAD0EB5DE49630C6588161&sugtime=1578274254338'
#代理机制对应的就是get或者post方法中的一个叫做proxies的参数
page_text = requests.get(url=url,headers=headers,proxies={'https':'14.134.184.156:39347'}).text


构建一个代理池，避免每次重复更换ip
import random
import requests
from lxml import etree

url = 'http://t.11jsq.com/index.php/api/entry?method=proxyServer.generate_api_url&packid=1&fa=0&fetch_key=&groupid=0&qty=21&time=1&pro=&city=&port=1&format=html&ss=5&css=&dt=1&specialTxt=3&specialJson=&usertype=2'

headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'
}

text_ip=requests.get(url=url,headers=headers).text
ip_list=[]
tree = etree.HTML(text_ip)
datas = tree.xpath('//body//text()')
for data in datas:
    dic ={
        'https':data
    }
    ip_list.append(dic)
print(ip_list)
```

##### 模拟登陆

###### 原因：相关的网络页面必须进过登陆之后才可以实现数据的展示

###### 验证码的处理：

1. 使用相关的打码平台进行验证码的动态识别

2. 打码平台推荐：

   1. 超级鹰：http://www.chaojiying.com/about.html
   2. 云打码

3. 超级鹰使用流程：

   1. 注册：必须以用户中心的身份账号

   2. 登陆：登陆之后点击创建一个软件

   3. 下载对应语言的示例代码

      ```Python
      #下载好的示例代码
      #!/usr/bin/env python
      # coding:utf-8
      
      import requests
      from hashlib import md5
      
      class Chaojiying_Client(object):
      
          def __init__(self, username, password, soft_id):
              self.username = username
              password =  password.encode('utf8')
              self.password = md5(password).hexdigest()
              self.soft_id = soft_id
              self.base_params = {
                  'user': self.username,
                  'pass2': self.password,
                  'softid': self.soft_id,
              }
              self.headers = {
                  'Connection': 'Keep-Alive',
                  'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)',
              }
      
          def PostPic(self, im, codetype):
              """
              im: 图片字节
              codetype: 题目类型 参考 http://www.chaojiying.com/price.html
              """
              params = {
                  'codetype': codetype,
              }
              params.update(self.base_params)
              files = {'userfile': ('ccc.jpg', im)}
              r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, files=files, headers=self.headers)
              return r.json()
      
          def ReportError(self, im_id):
              """
              im_id:报错题目的图片ID
              """
              params = {
                  'id': im_id,
              }
              params.update(self.base_params)
              r = requests.post('http://upload.chaojiying.net/Upload/ReportError.php', data=params, headers=self.headers)
              return r.json()
          
          
      # if __name__ == '__main__':
      # 	chaojiying = Chaojiying_Client('超级鹰用户名', '超级鹰用户名的密码', '96001')	#用户中心>>软件ID 生成一个替换 96001
      # 	im = open('a.jpg', 'rb').read()													#本地图片文件路径 来替换 a.jpg 有时WIN系统须要//
      # 	print chaojiying.PostPic(im, 1902)												#1902 验证码类型  官方网站>>价格体系 3.4+版 print 后要加()  
      ```

   4. 使用的时候自己要封装一个识别验证码的类

      ```Python
      def transformCode(imgPath,imgType):
          chaojiying = Chaojiying_Client('bobo328410948', 'bobo328410948', '899370')
          im = open(imgPath, 'rb').read()
          return chaojiying.PostPic(im,imgType)['pic_str']
      ```

###### 登陆时cookie的处理：

1. 手动处理：
   1. 将请求携带的cookie封装到headers中
2. 自动处理
   1. 通过session对象进行处理：该对象和requests都可以进行get和post的请求发送，如果使用session对象在进行请求发送过程中，产生了cookie，则cookie会自动保存在session对象中，如果cookie存储到了session对象中，再次使用session进行请求发送的时候，则这次请求的时候就是携带了cookie的请求
   2. 在使用session处理cookie的时候，session对象最少需要发送两次请求，第一次会获取和存储cookie，第二次请求才是携带了cookie的session请求

##### 遇到了动态变化的请求参数应如何处理：

1. 通常情况下，动态变化的请求参数的值都会被隐藏在前台页面中
2. 基于抓包工具进行全局搜索（基于js逆向获取相关的数据值）

##### 线程池：

安装和导包：from multiprocessing.dummy import Pool

操作：主要被应用在耗时的操作过程中

```
pool.map(callback,alist) #实例化一个这个线程池对象
让callback回调函数可以异步将alist中的列表元素进行某种形式的操作
```

​	注意事项：callback必须要有一个参数	

###### 开启异步操作：

```python
#数据的爬取，返回爬取到的页面源码数据
def get_request(url):
    page_text = requests.get(url=url).text
    return page_text

#数据解析
def parse(page_text):
    soup = BeautifulSoup(page_text,'lxml')
    return soup.select('#feng')[0].string
#异步代码
if __name__ == '__main__':
    start = time.time()

    pool = Pool(3)#实例化线程池对象
    #参数2这个回调函数需要接受参数1列表中的某一个列表元素，回调函数可以对该列表元素进行
        #指定形式的操作
    page_text_list = pool.map(get_request,urls)
    parse_text_list = pool.map(parse,page_text_list)
    print(parse_text_list)
```

###### 基于线程池爬取梨视频首页热点视频

```Python
import requests
from lxml import etree
import re
from multiprocessing.dummy import Pool
import random
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'
}
video_urls = []
main_url = 'https://www.pearvideo.com/category_3'
main_page_text = requests.get(url=main_url,headers=headers).text
#数据解析：每一个视频详情页的url
tree = etree.HTML(main_page_text)
li_list = tree.xpath('//*[@id="listvideoListUl"]/li')
for li in li_list:
    detail_url = "https://www.pearvideo.com/"+li.xpath('./div/a/@href')[0]
    page_text = requests.get(detail_url,headers=headers).text
    ex = 'srcUrl="(.*?)",vdoUrl='
    video_url = re.findall(ex,page_text,re.S)[0]
    video_urls.append(video_url)
# print(video_urls)

def get_video(url):
    video_data = requests.get(url=url,headers=headers).content
    fileName = str(random.randint(0,1000))+'.mp4'
    with open(fileName,'wb') as fp:
        fp.write(video_data)
    print(fileName,"下载成功！！！")
pool = Pool(4)
pool.map(get_video,video_urls)
```

##### 协程：

###### 开启一个协程

```Python
import asyncio
from time import sleep

#定义好了一个特殊的函数
async def get_request(url):
    print('正在请求:',url)
    sleep(2)
    print('请求成功：',url)
    return 'bobo'

#定义一个task的回调函数
def callback(task):
    print(' i am callback:',task.result())

#调用了特殊的函数，意味着该函数内部的实现语句不会被立即执行而是会返回一个协程对象
c = get_request('www.q.com')

#创建一个任务对象
task = asyncio.ensure_future(c)

#给任务对象绑定回调函数
task.add_done_callback(callback)

#创建事件循环对象
loop = asyncio.get_event_loop()
#将任务对象注册/装载到事件循环对象中，然后需要启动事件循环对象
loop.run_until_complete(task)
```

###### 开启多任务操作：

```Python
import asyncio
from time import sleep
import time

start = time.time()
#定义好了一个特殊的函数
async def get_request(url):
    print('正在请求:',url)
    await asyncio.sleep(2)
    print('请求成功：',url)
    return 'bobo'

urls = [
    'http://localhost:5000/bobo',
    'http://localhost:5000/jay',
    'http://localhost:5000/tom'
]

tasks = [] #多个任务对象
for url in urls:
    c = get_request(url)
    task = asyncio.ensure_future(c)
    tasks.append(task)

loop = asyncio.get_event_loop()
#将任务列表注册到事件循环中的时候一定要将任务列表进行挂起操作
loop.run_until_complete(asyncio.wait(tasks))

print('总耗时：',time.time()-start)
```

##### 单线程+多任务的异步协程

###### 特殊函数：

1. 如果一个函数的定义被async关键字修饰后，则该函数就是一个特殊的函数

   ```python
   async def 函数名(url):
       ……
   ```

2. 特殊之处：

   1. 该函数在被调用后函数内部的实现语句不会被立即执行
   2. 该函数会返回一个协程对象

###### 协程

是一个对象。当页数函数被调用后，该函数就会返回一个协程对象

```
协程对象==特殊函数
```

###### 任务对象

1. 就是对协程对象进行进一步的封装(就是一个高级的协程对象)

2. 任务对象==协程对象==特殊函数

   ```
   task = asyncio.ensure_future(参数)
   ```

3. 绑定回调：

   ```python
   task.add_done_callback(funcName)
   funName这个回调函数必须要带一个参数，这个参数表示的就是当前的任务对象
   参数.result():表示的就是当前任务对象对应的特殊函数的返回值
   ```

###### 事件循环对象

1. 创建事件循环对象

   ```Python
   asyncio.get_event_loop()
   ```

2. 需要将任务对象注册到该事件循环对象中且启动时间循环

   ```Python
   loop.run_until_complete(task)
   ```

###### 等待（await）：当阻塞操作结束后让loop回头执行阻塞之后的代码

```
async with await sess.get(url=url) as response:
```

###### 挂起（wait）：将当前的任务对象交出cpu的使用权

```Python
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
```

###### 注意事项：

在书写特殊函数的时候，不要在函数内部出现不支持异步操作的代码，否则就会中断整个异步操作的执行

##### aiohttp：

1. requests不支持异步操作，不可以出现在函数内部

2. aiohttp是一个支持异步的网络请求模块

   ```
   pip install aiohttp
   ```

3. 代码规范：

   ```python
   - 写出基本架构
   with aiohttp.ClientSession() as sess:
     #withsess.get/post(url=url,headers=headers,data/params,proxy="http://ip:port") as response:
   	with sess.get(url=url) as response:
           #text()：获取字符串形式的响应数据
           #read():获取bytes类型的响应数据
           page_text = response.text()
           return page_text
   - 补充细节
       - 在每一个with前加上async
       - 在每一个阻塞操作前加上await关键字
       - 代码参照完整代码
   ```

   aiohttp完整代码规范：

   ```python
   async with aiohttp.ClientSession() as sess:            #withsess.get/post(url=url,headers=headers,data/params,proxy="http://ip:port") as response:
       async with await sess.get(url=url) as response:
           #text()：获取字符串形式的响应数据
           #read():获取bytes类型的响应数据
           page_text = await response.text()
           return page_text
   ```

   一般我们解析带HTML的标签的文本内容使用bs4进行解析

###### 完整代码：

```python
import asyncio
from time import sleep
import time
import requests
from lxml import etree
import aiohttp
urls = [
    'http://localhost:5000/bobo',
    'http://localhost:5000/jay',
    'http://localhost:5000/tom'
]
start = time.time()
#发起请求获取响应数据（不可以实现异步）
# async def get_request(url):
#     #requests是不支持异步的模块
#     page_text = requests.get(url).text
#     return page_text

#基于aiohttp实现异步的网络请求
async def get_request(url):
    #实例化了一个请求对象
    async with aiohttp.ClientSession() as sess:
        #with sess.get/post(url=url,headers=headers,data/params,proxy="http://ip:port") as response:
        async with await sess.get(url=url) as response:
            #text()：获取字符串形式的响应数据
            #read():获取bytes类型的响应数据
            page_text = await response.text()

            return page_text

#定义回调函数
def parse(task):
    page_text = task.result()#获取特殊函数的返回值（请求到的页面源码数据）
    tree = etree.HTML(page_text)
    print(tree.xpath('//*[@id="feng"]/text()')[0])

tasks = []
for url in urls:
    c = get_request(url)
    task = asyncio.ensure_future(c)
    task.add_done_callback(parse)
    tasks.append(task)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

print('总耗时：',time.time()-start)
```

##### selenium：

###### 概念：基于浏览器自动化的模块

###### 安装：pip install selenium

###### 爬虫之间的关联性：

1. 可以实现模拟登陆
2. 便捷的帮我们捕获到动态加载的数据（可见既可得）

###### 基本操作：

1. 必须提供浏览器对应的驱动

2. 实例化一个浏览器对象

   ```python
   bro = webdriver.Chrome(executable_path='驱动程序的路径')
   ```

3. 定位标签：使用find进行查找

4. 向定制的标签内录入数据

5. 点击或其他操作

6. 注意：page_source：返回当前页面的页面源码数据（包含动态加载的数据）

7. 缺点就是效率太低
   

```
from selenium import webdriver
from time import sleep
#实例化浏览器对象
bro = webdriver.Chrome(executable_path='./chromedriver.exe')
#制定一些自动化的操作

#发起请求
bro.get('https://www.jd.com/')

#标签定位
search_tag = bro.find_element_by_id('key')
#向文本框中录入数据
search_tag.send_keys('mac pro')
sleep(2)
btn = bro.find_element_by_xpath('//*[@id="search"]/div/div[2]/button')
btn.click()
sleep(2)
#js注入execute_script('jsCode')
bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
sleep(2)
#page_source：返回当前页面的页面源码数据（包含动态加载数据）
print(bro.page_source)

#关闭浏览器
bro.quit()
```

###### aiohttp：支持异步的网络请求模块

1. 使用使用上下文机制（with...as）
2. 实例化一个请求对象（ClientSession()）
3. get/post（）进行请求发送。（阻塞操作）
   - proxy参数：'[http://ip:port](http://ip:port/)'
4. 获取响应数据（阻塞操作）
   - response.text():字符串
   - response.read():byte

###### 动作链：

1. 导入包：from selenium.webdriver import ActionChains
2. NoSuchElementException:没有定位到指定的标签
   1. 定位的标签是存在于一张嵌套的子页面中，如果想定位子页面中的指定标签的话需要：bro.switch_to.frame('iframe标签id的属性值')：将当前浏览器页面切换到指定的子页面的范围中
3. 针对指定的浏览器实例化一个动作链对象
   1. action = ActionChains（bro）
4. action.click_and_hold(tagName)
5. move_by_offset(10,15)
6. perform（）是的动作链立即执行

```python
from selenium import webdriver
from selenium.webdriver import ActionChains
from time import sleep
# 后面是你的浏览器驱动位置，记得前面加r'','r'是防止字符转义的
bro = webdriver.Chrome('./chromedriver.exe')
bro.get('https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable')

#标签定位
bro.switch_to.frame('iframeResult')
div_tag = bro.find_element_by_id('draggable')

#需要使用ActionChains定制好的行为动作
action = ActionChains(bro)#针对当前浏览器页面实例化了一个动作链对象
action.click_and_hold(div_tag)#点击且长按指定的标签

for i in range(1,7):
    action.move_by_offset(10,15).perform()#perform（）是的动作链立即执行
    sleep(0.5)
    
bro.quit()
```

###### 无头浏览器

1. 没有可视化界面的浏览器
2. phantomJS：无头浏览器
3. 谷歌无头浏览器：
   - 就是你本机安装的谷歌浏览器，只是需要通过代码进行相关配置就可以变成无头浏览器

```jpytuer
from selenium.webdriver.chrome.options import Options
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')



bro = webdriver.Chrome(executable_path='./chromedriver.exe',chrome_options=chrome_options)
bro.get('https://www.baidu.com/')
sleep(1)
bro.save_screenshot('./1.png')#截屏

print(bro.page_source)
```

###### selenium规避检测

1. 浏览器托管
   1. 环境配置：
      1. 本机谷歌浏览器驱动程序所在的目录的路径添加到环境变量中
      2. 使用本机谷歌的驱动程序开启一个浏览器
      3. chrome.exe --remote-debugging-port=9222 --user-data-dir="C:\selenum\AutomationProfile"

- 9222：端口（任意）

- "C:\selenum\AutomationProfile"：已经事先存在的一个空目录

- 使用如下代码接管目前打开的浏览器：

  ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.options import Options
   
  chrome_options = Options()
  chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
  
  
  bro = webdriver.Chrome(executable_path='./chromedriver.exe',chrome_options=chrome_options)#代码托管代开的浏览器，不会实例化一个新的浏览器。
  bro.get('https://kyfw.12306.cn/otn/login/init')
  ```

###### 12306模拟登陆：

分析：

1. 识别的验证码图片必须通过截图的方式然后存储到本地
2. 登陆操作和唯一的验证码图片一一对应

```python
#pip install Pillow
from PIL import Image
from selenium.webdriver import ActionChains
from selenium import webdriver
#识别验证码的函数
def transformCode(imgPath,imgType):
    chaojiying = Chaojiying_Client('13614167787', '13614167787', '903126')
    im = open(imgPath, 'rb').read()
    return chaojiying.PostPic(im,imgType)['pic_str']
```

```python
bro = webdriver.Chrome(executable_path='./chromedriver.exe')
bro.get('https://kyfw.12306.cn/otn/login/init')
sleep(2)
bro.find_element_by_id('username').send_keys('xxxxxxx')
bro.find_element_by_id('password').send_keys('12345465')

#验证码的点击操作
bro.save_screenshot('main.png')#将页面当做图片保存到本地
#将单独的验证码图片从main.png中裁剪下载
img_tag = bro.find_element_by_xpath('//*[@id="loginForm"]/div/ul[2]/li[4]/div/div/div[3]/img')#将验证码图片的标签定位到了
location = img_tag.location
size = img_tag.size
# print(location,size)
#裁剪的范围(验证码图片左下角和右上角两点坐标)
rangle = (int(location['x']),int(location['y']),int(location['x']+size['width']),int(location['y']+size['height']))
#使用Image类根据rangle裁剪范围进行验证码图片的裁剪
i = Image.open('./main.png')
frame = i.crop(rangle)#验证码对应的二进制数据
frame.save('./code.png')

result = transformCode('./code.png',9004)#99,71|120,140
#99,71|120,140 == [[99,71],[120,140]]
all_list = []#[[99,71],[120,140]]
if '|' in result:
    list_1 = result.split('|')
    count_1 = len(list_1)
    for i in range(count_1):
        xy_list = []
        x = int(list_1[i].split(',')[0])
        y = int(list_1[i].split(',')[1])
        xy_list.append(x)
        xy_list.append(y)
        all_list.append(xy_list)
else:
    x = int(result.split(',')[0])
    y = int(result.split(',')[1])
    xy_list = []
    xy_list.append(x)
    xy_list.append(y)
    all_list.append(xy_list)
    
for data in all_list:
    x = data[0]#11
    y = data[1]#22
    ActionChains(bro).move_to_element_with_offset(img_tag,x,y).click().perform()
    sleep(1)
    
sleep(2)
bro.find_element_by_id('loginSub').click()

bro.quit()
```

```python
#下载好的示例代码
#!/usr/bin/env python
# coding:utf-8

import requests
from hashlib import md5

class Chaojiying_Client(object):

    def __init__(self, username, password, soft_id):
        self.username = username
        password =  password.encode('utf8')
        self.password = md5(password).hexdigest()
        self.soft_id = soft_id
        self.base_params = {
            'user': self.username,
            'pass2': self.password,
            'softid': self.soft_id,
        }
        self.headers = {
            'Connection': 'Keep-Alive',
            'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)',
        }

    def PostPic(self, im, codetype):
        """
        im: 图片字节
        codetype: 题目类型 参考 http://www.chaojiying.com/price.html
        """
        params = {
            'codetype': codetype,
        }
        params.update(self.base_params)
        files = {'userfile': ('ccc.jpg', im)}
        r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, files=files, headers=self.headers)
        return r.json()

    def ReportError(self, im_id):
        """
        im_id:报错题目的图片ID
        """
        params = {
            'id': im_id,
        }
        params.update(self.base_params)
        r = requests.post('http://upload.chaojiying.net/Upload/ReportError.php', data=params, headers=self.headers)
        return r.json()


# if __name__ == '__main__':
# 	chaojiying = Chaojiying_Client('超级鹰用户名', '超级鹰用户名的密码', '96001')	#用户中心>>软件ID 生成一个替换 96001
# 	im = open('a.jpg', 'rb').read()													#本地图片文件路径 来替换 a.jpg 有时WIN系统须要//
# 	print chaojiying.PostPic(im, 1902)												#1902 验证码类型  官方网站>>价格体系 3.4+版 print 后要加()
```

##### js解密，混淆，逆向

###### 利用中国气象局天气状态分析：

- url:https://www.aqistudy.cn/html/city_detail.html
- 分析：
  - 空气指标的数据是动态加载出来
    - 修改了搜索条件后点击搜索按钮会发起ajax请求，请求到我们想要的指标数据。
  - 从上一步定位到的数据包中提取出url,请求方式，请求参数
    - url和请求方式可以拿来直接用
    - 请求参数是动态变化且加密
  - 响应数据也是加密的密文数据

###### 解题思路：

找到点击搜索按钮发起的ajax请求对应的代码

- 基于火狐浏览器的开发者工具找到搜索按钮对应点击事件绑定的js函数是哪个
  - getData(),该函数的实现
    - type=HOUR:以小时为单位进行数据的查询
    - 调用了另两个函数：getAQIData()， getWeatherData()
    - 并没有找到ajax请求对应的的代码
  - 分析getAQIData&getWeatherData：
    - 这两个函数的实现几乎一致，唯一的区别是
      - var method = 'GETDETAIL';
      - var method = 'GETCITYWEATHER';
    - 也没有找到ajax请求对应的代码，但是发现了另一个函数的调用：
      - getServerData(method, param, function(obj),0.5 )
        - method：
          - 'GETDETAIL'
          - 'GETCITYWEATHER'
        - param是一个字典，有四组键值对：
          - city;
          - type;
          - startTime;
          - endTime;
    - 分析getServerData函数的实现：
      - 基于抓包工具进行全局搜索，定位到了一个指定的数据包，出现了getServerData关键词，这个关键词对应的js代码被加密了
      - JS混淆：将js中的核心代码加密
      - JS反混淆：
        - 暴力破解：
          - url:https://www.bm8.com.cn/jsConfusion/
        - 分析反混淆后的getServerData函数的实现代码：
          - 终于发现了ajax请求对应的代码：
            - getParam(method, object)返回动态变化且加密的请求参数d的值。
              - method == method
              - object == param
            - decodeData(data)：接受加密的响应数据返回解密后的明文数据
              - data:加密的响应数据

###### js逆向分类：

- 自动逆向：
  - PyExecJS介绍：PyExecJS 是一个可以使用 Python 来模拟运行 JavaScript 的库。
    - 一定实现在本机装好nodejs的开发环境
  - 我们需要pip install PyExecJS对其进行环境安装。

```Python
#模拟执行js函数获取动态变化且解密的请求参数d的值
import execjs
node = execjs.get()
 
# Params
method = 'GETCITYWEATHER'
city = '北京'
type = 'HOUR'
start_time = '2018-01-25 00:00:00'
end_time = '2018-01-25 23:00:00'
 
# Compile javascript
file = 'test.js'
ctx = node.compile(open(file,encoding='utf-8').read())
 
# Get params
js = 'getPostParamCode("{0}", "{1}", "{2}", "{3}", "{4}")'.format(method, city, type, start_time, end_time)
params = ctx.eval(js)#模拟执行指定的js函数
print(params)
```

```Python
import execjs
import requests

node = execjs.get()
 
# Params
method = 'GETCITYWEATHER'
city = '北京'
type = 'HOUR'
start_time = '2018-01-25 00:00:00'
end_time = '2018-01-25 23:00:00'
 
# Compile javascript
file = 'test.js'
ctx = node.compile(open(file,encoding='utf-8').read())
 
# Get params
js = 'getPostParamCode("{0}", "{1}", "{2}", "{3}", "{4}")'.format(method, city, type, start_time, end_time)
params = ctx.eval(js)

#发起post请求
url = 'https://www.aqistudy.cn/apinew/aqistudyapi.php'
response_text = requests.post(url, data={'d': params}).text
print(response_text)
```

```Python
#将密文的响应数据进行解密：模拟调用decodeData(data)
import execjs
import requests

node = execjs.get()
 
# Params
method = 'GETCITYWEATHER'
city = '北京'
type = 'HOUR'
start_time = '2018-01-25 00:00:00'
end_time = '2018-01-25 23:00:00'
 
# Compile javascript
file = 'test.js'
ctx = node.compile(open(file,encoding='utf-8').read())
 
# Get params
js = 'getPostParamCode("{0}", "{1}", "{2}", "{3}", "{4}")'.format(method, city, type, start_time, end_time)
params = ctx.eval(js)

#发起post请求
url = 'https://www.aqistudy.cn/apinew/aqistudyapi.php'
response_text = requests.post(url, data={'d': params}).text
# #对加密的响应数据进行解密
jss = 'decodeData("{0}")'.format(response_text)
print(jss)
decrypted_data = ctx.eval(jss)
print(decrypted_data)
```

#### scrapy框架：

框架：就是一个具有很强的通用性切封装一些通用实现方法（功能）的一个项目模版

##### scrapy（异步）

1. 高性能的网络请求
2. 高性能的数据解析
3. 高性能的持久化存储
4. 全站数据爬取
5. 深度爬取
6. 分布式

##### 安装：

1. pip install wheel
2. 在这个网址下载一个与你当前操作系统对应的twisted：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
3. 进入下载好的文件目录中（twisted）：执行 pip install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl
4. 安装pip install pywin32
5. pip install scrapy

##### 创建一个工程文件：

###### 命令行：

1. scrapy startproject 文件名
2. 必须切换到当前文件目录下：cd 文件名
3. 创建一个爬虫文件：scrapy genspider 文件名 url地址

###### 基本配置：在settings.py

1. UA伪装
2. robots协议的不遵从
3. 指定日志等级：LOG_LEVEL = 'ERROR'

```python
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'
ROBOTSTXT_OBEY = False
LOG_LEVEL = 'ERROR'
```



###### 启动：

scrapy crawl 文件名

###### 数据解析：

1. response.xpath('xpath表达式')

2. 与etree的不同之处：

   1. 取文本/属性：返回的是一个Selector对象，文本数据是存储在该对象中
   2. 可以调用extract()/extract_first()取出字符串数据

3. 常用操作：

   1. 直接使用xpath返回列表调用extract()/extract_first()
   2. 如果列表元素只有一个则调用extract_first()，返回的字符串
   3. 如果列表元素为多个则调用extract()，返回的是列表，列表里装的是字符串

4. 持久化存储：

   1. 基于终端指令的持久化存储

      1. 只可以将parse方法的返回值存储到指定后缀（csv）文本文件中
      2. 指令：scrapy crawl 文件名 -o filePath

   2. 基于管道的持久化存储

      1. 实现流程：

         1. 数据解析（爬虫文件）

         2. 在items类中定义相关的属性（items.py）:fieldName = scrapy.Field()

         3. 将解析的数据存储封装到item类型的对象中（爬虫文件）：item['fieldName'] = value #给item对象的fieldName属性赋值

         4. 将item对象提交给管道（爬虫文件）：yield item #只可以将item提交给优先级最高的管道

         5. 在管道中接收item文件可以将item中存储的数据进行任意形式的持久化存储（pipelines.py）：process_item():负责接收item对象且对其进行持久化存储

         6. 在配置文件中开启管道机制：

            ```
            ITEM_PIPELINES = {
             #300：表示的是优先级，数值越小优先级越高
             'duanziPro.pipelines.DuanziproPipeline': 300,}
            ```

         7. 细节处理：

            1. 管道文件中的管道类表示的是什么？
               1. 一个管道类对应的就是一种存储形式
               2. 如果想要实现数据备份，则需要使用多个管道类（多种存储形式：数据库/redis）
            2. process_item中的return item：
                1. 将item传递给下一个即将被执行的管道类

###### 手动get请求发送：

1. yield scrapy.Request(url,callback)
   1. url:指定好的请求的url
   2. callback：callback指定的回调函数一定会被执行（数据解析）

2. post请求的发送：
   1. yield scrapy.FormRequest(url,callback,formdata)

###### yield在scrapy中的使用：

1. 向管道提交item对象：yield item
2. 手动请求发送：yield scrapy.Request(url,callback)

###### 问题：如何在start_urls中的元素进行post请求的发送：

1. 重写start_requests方法

   ```python
   def start_requests(self):#父类方法
       for url in self.start_urls:
           #发起get请求
           yield scrapy.FormRequest(url=url,callback=self.parse,formdata)
   ```

###### scrapy的五大核心组件：

1. 引擎（Scrapy）：

   用来处理整个系统的数据流处理，触发事务（框架核心）

2. 调度器（Scheduler）：

   用来接受引擎发过来的请求，亚茹队列中，并在引擎再次请求的时候返回，可以想象成一个URL（抓取网页的地址或者说是链接）的优先队列，由他来决定下一个要抓取的网址是什么，同时去除重复的网址

3. 下载器（Downloader）：

   用于下载网页内容，并将网页内容返回给蜘蛛（Scrapy下载器是建立在twisted这个高效的异步模型上的）

4. 爬虫（Spidres）：

   爬虫是主要干活的，用于特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面

5.  项目管道(Pipeline)
   负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。