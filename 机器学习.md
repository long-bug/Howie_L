### 机器学习

##### 概述

概念：机器学习就是从“数据”中自动分析获得“规律（模型）”，并利用规律对未知数据进行预测

###### 模型：

特殊对象-该对象内部已经集成或者封装好了某种形式的算法或者（方程：没有求出解的方程）

###### 模型的作用：

模型对象内部的方程式求出的解就是分类或者预测的结果！！（**重要**）

-    预测
-    分类

###### 样本数据：

- 样本数据和模型对象之间的关联
  - 需要将样本数据代入到模型中的方程式中，对其进行求解操作，那么方程求出的解就是预测或者分类的结果
- 训练模型：
  - 将样本数据代入模型中，然后对方程式进行求解
  - 只有保证数模型被训练完毕之后，才可以实现预测或者分类的功能

###### 样本数据的组成部分：

- 特征数据：自变量（二维）
- 目标数据：因变量（不限制维度）

###### 模型的分类：

- 有监督学习：如果训练模型使用的样本数据必须包含特征数据和目标数据，则该模型属于有监督学习类
- 无监督学习：只需要使用特征数据就行（k-means）

###### 样本数据（数据集）的载体：

- 通常情况下历史数据都不会存储在数据库中，而是存储在文件中（.csv文件）
- 数据库存储数据存在的问题：
  - 性能瓶颈：数据量过大时，数据库很难支持数据进行同时读写
  - 数据存储格式不符合机器学习要去的数据格式

###### 样本数据获取的途径：

- kaggle
- UCI数据集：是一个常用的机器学习标准测试数据集，是加州大学欧文分校提出的用于机器学习的数据库
- sklearn

###### 为什么学习机器学习：

- 解放生产力：比如智能客服（不知疲倦24小时工作，还不用支付其工资）可以代替人工客服。
- 解决专业问题：比如医疗方面的【ET医疗】，可以辅助医生进行相关症状的判断，数据显示，人类医生的平均准确率为60%-70%，而当下算法的准确率已经达到85%。
- 提供社会便利：杭州的城市大脑，无人超市等等。

###### 机器学习的价值体现：

- 价值体现在各个方面：医疗，航空，教育，物流，电商......
- 让机器学习程序替换手动的步骤，减少企业的成本也提高企业的效率。
- 例子：汽车销售在对新车进行推广的时候，一般都是将宣传手册投放给了所有的客户，从中进行目标客户的定位。但是如果使用机器学习对客户进行指定分类，然后将手册可以根据分类投放到不同类别的客户中，则这样就大大增加了商业机会。



##### 特征工程：

###### 什么是特征工程：

特征工程是将原始数据转换为更好的代表预测模型的潜在问题的特征的过程，从而提高对未知数据预测的准确性

###### 为什么需要特征工程：

- 样本数据中可能会存在一些异常值，缺失值，重复值等等，这时候我们就需要对样本数据进行相关的噪点数据的处理，处理的目的就是为了使数据更加纯净，从而得到一个更加纯净的样本集，从而使模型可以基于这组数据可以有更好的预测能力
- 比如AlphaGo学习的数据中既有棋谱，又有食谱还有歌词，那么一些干扰的数据绝对会影响AlphaGo的学习。

###### 特征工程的意义：

- 直接影响模型预测的结果

###### 如何实现特征工程：

- 借助于工具：sklearn

###### sklearn介绍

- 是python语言中的机器学习的工具，包含了很多知名的机器学习算法的实现，其文档完善，容易上手
- 功能：
  - 分类模型
  - 回归模型
  - 聚类模型
  - 特征工程



##### 特征抽取：

###### 目的：

我们所采取到的样本中的特征数据往往是一些字符串或者其他类型的数据，而电脑识别的二进制数据，如果我们把采取到的特征数据给电脑，电脑是不识别的。

```python
#导入一个API接口，这是jupyter事先封装好的，使用这个类可以使字符串数据类型转换成数值类型
from sklearn.feature_extraction.text import CountVectorizer
vector = CountVectorizer()
# 调用这个方法
res = vector.fit_transform(['lift is short,i love python','lift is too long,i hate python'])
print(res.toarray())
```

结果：![image-20200203171840138](F:\Python学习笔记\图片\image-20200203171840138.png)

结论：特征抽取对文本等数据进行特征值化，特征值化是为了让机器更好的理解数据



###### 字典特征抽取：

作用：对字典类型的数据进行特征值化

API：

```python
from sklearn.feature_extraction import DictVectorizer
```

- fit_transform(X):X为字典或者包含字典的迭代器，返回值为sparse矩阵
- inverse_transform(X)：X为sparse矩阵或者array数组，返回值为转换之前的数据格式
- transform(X)：按照原先的标准转换
- get_feature_names()：返回类别名称

```
alist = [
            {'city':'BeiJing','temp':33},
            {'city':'GZ','temp':42},
            {'city':'SH','temp':40}
        ]
        
        
from sklearn.feature_extraction import DictVectorizer
#实例化字典特征的类
t = DictVectorizer()
# 将需要特征值化的数据作为参数传递到该方法中去
result =t.t_transform(alist) 
返回的是一个sparse矩阵
```

![image-20200203173232486](F:\Python学习笔记\图片\image-20200203173232486.png)

什么是sparse矩阵？

- 在字典特征值实例化的类中，将类中的构造方法设定为sparse=False，则返回的就不是一个sparse矩阵，而是一个数组

- get_feature_names()：返回类别名称

- sparse矩阵就是一个变相的数组或者列表，目的是为了节省内存

  ```python
  #将sparse数组转换成我们更好理解的数组
  from sklearn.feature_extraction import DictVectorizer
  #设定sparse=False，返回的就不是一个sparse矩阵了，是一个数组
  d = DictVectorizer(sparse=False) #sparse=False返回的特征值化的结果就不是sparse矩阵了
  result = d.fit_transform(alist)
  #返回的是特征的名称
  print(d.get_feature_names())
  print(result)
  ```

  ![image-20200203173710611](F:\Python学习笔记\图片\image-20200203173710611.png)

##### OneHot编码：

sparse矩阵中的0或者1就是OneHot编码（0表示无，1表示有）

###### 为什么需要OneHOt编码：

特征抽取主要是目的是对非数值型的数据进行特征值化，

###### 基于panadas实现OneHot编码：

pd.get_dummies(df['color'])

```
import pandas as pd
df = pd.DataFrame([
 ['green', 'M', 20, 'class1'],
 ['red', 'L', 21, 'class2'],
 ['blue', 'XL',30, 'class3']])
df.columns = ['color', 'size', 'weight', 'class label']
df1 = pd.get_dummies(df['color'])
pd.concat((df,df1),axis=1).drop('color',axis=1)
```

![image-20200203181311459](F:\Python学习笔记\图片\image-20200203181311459.png)

##### 文本特征抽取：

作用：对文本数据进行特征值化

API:from sklearn.feature_extraction.text import CountVectorizer

- fit_transform(X):X为文本或者包含文本字符串的可迭代对象，返回sparse矩阵

- inverse_transform(X)：X为array数组或者sparse矩阵，返回转换之前的格式数据

- get_feature_names()

- toarray()：将sparse矩阵换成数组

  ```
  from sklearn.feature_extraction.text import CountVectorizer
  text = ['我喜欢吃苹果你呢你喜不喜欢吃呢','我是一个好人你是不是呢']
  c = CountVectorizer()
  result = c.fit_transform(text)
  #将返回的sparse矩阵转换成二维数组
  result = result.toarray()#toarray()将sparse矩阵转换成二维的列表
  print(c.get_feature_names())
  print(result)#单独的一个汉字是无法表示文本特征的
  #分词：将一段文本中的词汇特征单独抽取出来
  
  结果：
  ['我喜欢吃苹果你呢你喜不喜欢吃呢', '我是一个好人你是不是呢']
  [[1 0]
  [0 1]]
  ```

  ```
  from sklearn.feature_extraction.text import CountVectorizer
  text = ['我,喜欢吃苹果 你呢？你喜不喜欢吃呢？','我,是一个好人~你是不是呢？']
  c = CountVectorizer()
  result = c.fit_transform(text)
  #将返回的sparse矩阵转换成二维数组
  result = result.toarray()#toarray()将sparse矩阵转换成二维的列表
  print(c.get_feature_names())
  print(result)#单独的一个汉字是无法表示文本特征的
  #分词：将一段文本中的词汇特征单独抽取出来
  
  结果：
  ['你呢', '你喜不喜欢吃呢', '你是不是呢', '喜欢吃苹果', '是一个好人']
  [[1 1 0 1 0]
  [0 0 1 0 1]]
  ```



##### 对中文文章进行分词处理我们使用jieba：

下载：pip install jieba

作用：对中文文章进行分词处理，一般我们需要将一段中文文本中相关的词语，成语，形容词进行抽取

###### 基本使用

```
import jieba
from sklearn.feature_extraction.text import CountVectorizer
text = ['新京报快讯3日，中国建筑集团在其官博发布武汉火神山医院病房内部实拍图，一起来看']
c = CountVectorizer()
result = c.fit_transform(text)
print(c.get_feature_names())
print(result)
```

![image-20200203182729812](F:\Python学习笔记\图片\image-20200203182729812.png)

```
#jieba分词的基本使用
text1 = '新京报快讯3日，中国建筑集团在其官博发布武汉火神山医院病房内部实拍图，一起来看，医院'
text2 = '武汉火神山医院病房内部长啥样？实拍图来了！医院，医院'

jb1 = jieba.cut(text1)#分词
jb_text1 = ' '.join(list(jb1))#将分词的结果拼接成一个字符串，词和词之间空隔一定要有

jb2 = jieba.cut(text2)
jb_text2 = ' '.join(list(jb2))

# print(jb_text) #显示分词拼接后的结果

c = CountVectorizer()
result = c.fit_transform([jb_text1,jb_text2]) #特征值化
print(c.get_feature_names())
print(result.toarray())
```

![image-20200203182747333](F:\Python学习笔记\图片\image-20200203182747333.png)



##### 特征的预处理：对数值型的数据进行处理

###### 含义：

特征抽取之后我们就可以获取对应的数值型的样本数据，通过样本数据我们就可以进行数据处理了

###### 概念：

通过特定的统计方法（数学方法），将数据转换成算法要求的数据

###### 方式：

- 归一化：如果认为每一个特征具有同等大小，权重也同等重要，则必须要进行归一化的处理
- 标准化（推荐）

###### 归一化的实现：

![image-20200203183113774](F:\Python学习笔记\图片\image-20200203183113774.png)

API：from sklearn.preprocessing import MinMaxScaler

- 参数：feature_range表示缩放范围，通常使用(0,1)

作用：是的某一个特征不会对最后的数据结果产生很大的影响

```
data = [
    [12345,2,0.3],
    [23432,6,0.6],
    [32141,9,0.7],
]
from sklearn.preprocessing import MinMaxScaler
mm = MinMaxScaler()
result = mm.fit_transform(data) #归一化
print(result)
```

![image-20200203183229610](F:\Python学习笔记\图片\image-20200203183229610.png)

如果数据中存在的异常值较多，会对结果造成什么影响？

- 在原始数据中我们可以看出如果异常值存在为数据中较大的值或者是较小的值，则对归一化之后的值影响较大，这个也是归一化之后的弊端，我们也没办法很好的处理异常值

总结：在不同的数据集中，数据集中的最大值和最小值是不断变化的，另外最大值和最小值也特别容易受到异常值的影响，所以归一化的方式具有一定的局限性



###### 标准化的实现：

使用标准化处理之后，每列所有的数据都聚集在均值为0，标准差为1的范围附近

![image-20200203183643905](F:\Python学习笔记\图片\image-20200203183643905.png)

API：

- 处理后，每列所有的数据都聚集在均值为0，标准差为1范围附近
- 标准化API:from sklearn.preprocessing import StandardScaler
  - fit_transform(X):对X进行标准化
  - mean_：均值
  - var_:方差

```
data = [
    [12345,2,0.3],
    [23432,6,0.6],
    [32141,9,0.7],
]
from sklearn.preprocessing import StandardScaler
s = StandardScaler()
result = s.fit_transform(data)#标准化之后的结果获取
print(result)
```

![image-20200203190542536](F:\Python学习笔记\图片\image-20200203190542536.png)

总结：在足够多的的数据样本条件支持下比较稳定，适合在数据量复杂的大数据场景下使用



##### 数据降维：

降维：降维中的维度指的不是数组的维度，是我们样本数据中的特征数量，比如一个样本数据中有4个不同的特征，则该样本数据的维度是4

![image-20200203190838872](F:\Python学习笔记\图片\image-20200203190838872.png)

###### 降维的方式

- 特征分析
- 主成分分析

##### 特征选择：从特征数据中选择部分数据作为最终机器学习中训练的数据

###### 原因：

1. 冗余：部分数据特征的相关度高，这样的数据容易消耗计算机的性能
2. 噪点：部分特征对预测的结果有偏执的影响

###### 实现：

1. 人为的对不相关的特征进行主观的舍弃
2. 在已有特征和对应预测结果分析的基础上，使用相关的工具过滤掉无用的或者权重比其他特征小的特征
   - 工具：Filter（过滤式）

###### 过滤式：

1. 原理：主要是根据基于特征列的方差来决定是否将特征进行舍弃，因为方差的大小表示特征值和平均值的偏离度，方差越小则表示特征列中的特征变化复读越小，则该特征对结果的影响就越小

2. API：from sklearn.feature_selection import VarianceThreshold

   1. VarianceThreshold(threshold=x):threshold方差的值，删除所有方差低于x的特征，默认值为0表示保留所有方差为非0的特征
   2. fit_transform(X):X为特征

   ```
   from sklearn.feature_selection import VarianceThreshold
   #特征数据
   feature = [
       [45,66,88,1,6],
       [11,22,97,6,1],
       [51,38,44,2,11],
   ]
   vt = VarianceThreshold(threshold=50) #过滤式工具过滤那些特征的方差小于50的特征
   result = vt.fit_transform(feature) #过滤特征
   print(result)
   
   结果为：
   [[45 66 88]
    [11 22 97]
    [51 38 44]]
   ```



##### 主成分分析PCA:是一种分析，简化数据集的技术

###### 大致原理：使用协方差矩阵来实现使用低维度的事物在误差最小的情况下表示高维度的事物 

###### 目的：

特征数量较大时，考虑到数据的优化，我们可以使数据维度压缩，尽可能的降低源数据的维度，也就是特征的复杂度，以此来达到数据损失较小。

###### 作用：

可以削减回归分析或者聚类分析中特征的数量

![image-20200203192405508](F:\Python学习笔记\图片\image-20200203192405508.png)

###### API：

- from sklearn.decomposition import PCA
- pca = PCA(n_components=None)
- n_components可以为小数（保留特征的百分比），整数（减少到的特征数量）
- pca.fit_transform(X)

```
from sklearn.decomposition import PCA
feature = [
        [45,66,88,1,6],
        [11,22,97,6,1],
        [51,38,44,2,11],
]
pca = PCA(n_components=3)
result = pca.fit_transform(feature)
print(result)

结果为：
[[-4.22622303e+00  2.80242977e+01  2.03625386e-15]
 [ 3.65893599e+01 -1.14356591e+01  2.03625386e-15]
 [-3.23631368e+01 -1.65886386e+01  2.03625386e-15]]
```

 

##### sklearn的数据集

###### 数据集的划分：

原因：机器学习就是从数据中自动分析获得规律，并利用规律对未知数据进行预测。（我们的模型一定是要经过样本数据对其进行训练，才可以对未知的数据进行预测）

在我们得到全部的数据后，并不是将所有的数据都用来进行训练模型，在我们需要评估当前模型的好坏时，需要一组新的数据进行评估，而不是使用原本的数据

样本数据的拆分：

- 训练集：训练模型（占比大于测试集占比）
- 测试集：评估模型--不同的类型的模型对应的评估方式也不一样

API：from sklearn.model_selection import train_test_split

- 方法：train_test_split(x,y,test_size,random_state)

- 参数：

          - x：特征
          - y：目标
          - test_size：测试集的比例
          - random_state：打乱的随机种子

- 返回值：训练特征，测试特征，训练目标，测试目标

###### 数据集的接口介绍：

- sklearn.datasets.load_*():获取小规模的数据集

- sklearn.datasets.fetch_*(data_home=None,subset):获取大规模的数据集data_home表示数据集下载目录,None为默认值表示的是家目录/scikit_learn_data（自动创建该文件夹）下。需要从网络下载.subset为需要下载的数据集，可以为train，test，all

  ```
  #获取小规模的样本数据（数据集）
  import sklearn.datasets as datasets
  data = datasets.load_iris()
  feature = data['data'] #提取出了特征数据
  target = data['target']#提取出来的目标数据
  ```

  ```
  使用feature.shape可以查看当前数据的条数
  ```

  ```
  data['target_names'] #目标数据的表示
  data['feature_names']#特征数据的表示
  ```

- 将提取出来的样本数据进行拆分

  ```python
  from sklearn.model_selection import train_test_split
  x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=0.2,random_state=10)
  
  
  #参数设置：
  #x_train.shape #训练集的特征数据的个数
  #x_test.shape#测试集的特征数据的个数
  #y_test #测试数据的目标数据的个数
  ```

  

##### 基础：

在机器学习中，算法是机器学习的核心，数据是计算的基础。

###### 数据类型分为：

1. 离散型数据：
   - 由记录不同类别个体的数目所得到的的数据，又称为计数数据，所以得到的全部数据都是整数，而且数据不能细分，也不能进一步提高数据之间的精准度
2. 连续型数据：
   - 变量可以在某一范围内取得的任意数，即变量的取值是连续的，比如长度，高度，时间，质量等，这些数据通常包含整数和小数
3. 总结：离散型是区间不可分的，连续性是区间可分的

###### 数据类型的不同应用：

在我们通常使用的图像识别和文章分类等对应数据量中可以使用离散型的数据表示，

###### 分类：

1. 有监督的学习
   1. 分类算法：KNN，贝叶斯，决策树，随机森林，逻辑回归等
   2. 回归算法：线性回归，岭回归
2. 无监督的学习：
   1. 聚类，K-means

###### 分类和回归的问题：

1. 分类算法基于的是目标数据为离散型的数据
2. 回归算法基于的是目标数据为连续性的数据
3. 在社会中产生的数据必然是离散型或者连续性，那么我们解决问题的方法也就是分类问题或者回归问题

##### 机器学习的开发流程：

1. 数据采集
   1. 公司内部产生的数据
   2. 和其他公司合作获取的数据
   3. 购买的数据
2. 分析数据所对应要解决需求或者问题是什么？根据目标数据推断问题属于分类问题还是回归问题！
3. 数据的基本处理
   1. 数据清洗
   2. 合并
   3. 级联
4. 特征工程：对特征进行处理
   1. 特征抽取
   2. 特征预处理
   3. 降维等
5. 选择合适的模型，然后对其进行训练
6. 模型的评估
7. 上线使用

##### KNN分类模型：

###### 概念：

简单来说，K-临近算法是基于相似度或者采用测量不同特征值之间的距离方法进行分类（k-Nearest Neighbor，KNN）

![image-20200204183849482](F:\Python学习笔记\图片\image-20200204183849482.png)

```
坐标系中点到点之间的距离A(x1,y1),B(x2,y2)
((x1-x2)2 + (y1-y2)2)**0.5
x这个位置分类的点到w1，w2，w3这三个类群之间的距离？
w1这个类群如果有10个散点组成，那么x到w1类群的距离就是x和10个散点之间距离的和！！！
基于上述模式计算距离分类有没有什么问题？
已知类群中的散点的个数不同的话会影响最终计算出来的距离累和
```

###### k值：

k值的作用：定义的k值不同会影响最终分类结果的不同！！！

- 需要找出未知分类散点周围最近的k个散点（近邻），观察这k个散点所对应的类群，如果某个类群对应的散点在k个散点中出现的次数最多，则该未知分类的散点就属于该类别！！！

![image-20200204183958454](F:\Python学习笔记\图片\image-20200204183958454.png)

###### 工作原理：

存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据 与所属分类的对应关系。输人没有标签的新数据后，将新数据的每个特征与样本集中数据对应的 特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们 只选择样本数据集中前K个最相似的数据，这就是K-近邻算法中K的出处,通常K是不大于20的整数。 最后 ，选择K个最相似数据中出现次数最多的分类，作为新数据的分类。

###### KNN分类原理总结：

1. 计算未知分类事物和已知分类事物之间的距离
2. 定义k值
3. 根据k值找出未知分类事物周围最近的k个已知分类的事物
4. 评判最近的k个已知分类事物对应的类别哪些事物出现的次数最多
5. 找出的类别就是对未知事物分类的结果

###### 优缺点：

- 优点
  - 分类效果显著（精准）
- 缺点
  - 慢

###### 算法模型怎么学？

1. 当前学习的模型使用来解决回归问题还是分类问题
2. 算法模型的实现原理是什么



##### 在scikit-learn库中使用k-近邻算法：

###### 分类问题：from sklearn.neighbors import KNeighborsClassifier

```python
#对电影进行分类
#1.捕获样本数据
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split #拆分样本数据
data = pd.read_excel('./datasets/my_films.xlsx')
#提取特征数据
feature = data[['Action Lens','Love Lens']]
#提取目标数据
target = data['target'] #存储的是电影的类型为非数值型数据，并且不需要对其进行任何其他操作。
#重点：在KNN中目标数据是不需要参与运算

#2.实例化算法模型对象
#n_neighbors:就是knn中的k值
knn = KNeighborsClassifier(n_neighbors=3)

#3.对样本数据进行拆分（训练集和测试集）
x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=0.2,random_state=20)
#训练集：x_train（训练集中的特征数据），y_train(训练集中的目标数据)
#测试集：x_test(测试集中的特征数据)，y_test(测试集中的目标数据)

#4.使用样本数据训练模型（使用训练集训练模型）
#fit(X,y):使用训练集训练模型,X表示的训练集中的特征数据，y表示的是训练集中的目标数据
#为什么参数X是大写的：提醒我们训练集的特征数据必须是二维的
knn.fit(x_train,y_train) 

#5.测试模型
#predict(X):表示使用模型来实现预测或者分类的功能，该方法返回的就是模型预测或者分类的结果
#参数X：测试集的特征数据，该方法返回的就是模型对测试数据分类的结果
pre_result = knn.predict(x_test) #模型对测试数据分类的结果
print('模型分类的结果：',pre_result)
print('测试数据真实的分类结果：',y_test.values) #测试集的目标数据就是测试数据的真实分类结果
```

```python
import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
#1.通过sklean获取鸢尾花对应的原始数据
data = datasets.load_iris()
#2.将鸢尾花原始数据抽取出来了特征数据和目标数据
feature = data['data']
target = data['target']
#3.查看目标和特征数据的形状
print(feature.shape) #特征数据是一个二维的数组
print(target.shape)  #目标数据是一个一维的数组
#4.查看特征数据的样式
# print(feature)#分析：1.特征数据需不需要进行相关的特征工程的处理。2.数据中是否存在缺失值
    #1.所有的特征数据为数值型的数据，因此不需要做特征抽取（特征值化）
    #2.查看特征的原始数据发现特征与特征之间相差的数据量级不大，因此也不需要做特征的预处理（标准化）保留该操作！
    #3.特征的维度为4，不需要进行降维
    #结论：如果最终训练好的模型测试效果不好，则可以考虑对特征进行标准化处理
#5.样本数据进行拆分
#训练集：x_train,y_train
#测试集：x_test, y_test
x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=0.2,random_state=20)

#6.实例化模型对象,指定了k值
knn = KNeighborsClassifier(n_neighbors=15)

#7.训练模型
knn.fit(x_train,y_train)#使用训练集的数据训练模型

#8.测试模型（使用测试集测试模型的精确度）
print('模型的分类结果：',knn.predict(x_test))
print('真实的分类结果：',y_test)

#使用精确度来测试模型
print('模型的精确度：',knn.score(x_test,y_test))
```

###### score表示计算模型的精确度：

1. knn.score(X,y)
   - X:二维形式的测试集的特征数据
   - y：测试集的目标数据
2. 返回值：
   - 测试模型精确度的百分比

```python
预测年收入是否大于50K美金

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

#1.读取原始数据
data = pd.read_csv('./datasets/adults.txt')
data.head()

#2.从原始数据中提取特征数据和目标数据
target = data['salary']
feature = data[['age','education_num','occupation','hours_per_week']]#将年龄，学历，工作，每周的工作时长

#3.查看特征和目标的形状
print('特征数据：',feature.shape)
print('目标数据：',target.shape)

#4.特征工程
    #1.occupation这个特征对应的数据为字符串，需要将其进行特征值化（one-hot编码）
        #one-hot编码：pd.get_dummies(feature['occupation'])
        #将one-hot编码之后的结果替换原始特征中的occupation这一个特征

feature = pd.concat([feature,pd.get_dummies(feature['occupation'])],axis=1)
#将occupation这一列从特征数据中删除
feature = feature.drop(labels='occupation',axis=1)

    #2.特征的预处理（归一化，标准化）【保留：如果模型测试的结果不好，在实现这一步】
s = StandardScaler() 
feature = s.fit_transform(feature)#使用标准化对特征进行了预处理，将预处理之后的结果在赋值给feature

#5.拆分数据
x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=0.1,random_state=23)

#6.实例化模型对象
knn = KNeighborsClassifier(n_neighbors=250)

#7.使用训练集训练（x_train，y_train）模型
knn.fit(x_train,y_train)

#8.测试模型
print('模型的精确度：',knn.score(x_test,y_test))

```

```python
#使用训练好的模型进行一次分类
    #获取一组测试数据
x = x_test[6]#获取某一个人的相关特征
x = x.reshape((1,-1)) #将测试数据变形成二维
knn.predict(x)#将这个人的特征进行分类，查看这个人的年收入是否大于50k
```

###### 导致模型测试结果不好的原因：

1. k的取值
2. 特征工程是否应用
3. 样本数据拆分的比例
4. 训练数据的数据量级

###### 如何将训练好的模型进行保存：

1. 整个模型实现的过程中最耗时的就是训练模型
2. API：
   1. from sklearn.externals import joblib
   2. joblib.dump(value,filename):模型保存的方法
      - value：训练好的模型对象
      - filename：将模型保存的位置
   3. 加载保存好的模型使用：
      - joblib.load('模型的路径')返回读取出来的模型对象（训练好的模型对象）

###### 如何合理进行k的取值：

1. k值可以被称为超参数
2. 超参数：算法模型对象实例化过程中需要使用到的参数且该参数值的不同会影响模型的精准度，那么这些参数就被称为超参数

##### k的取值问题：交叉验证选取k值：

1. k的取值较小的时候则模型的复杂度较高，数据容易发生拟合，学习的估计误差会很大，预测结果对近邻的实例点非常敏感
2. K值较大可以减少学习的估计误差，但是学习的近似误差会增大，与输入实例较远的训练实例也会对预测起作用，使预测发生错误，k值增大模型的复杂度会下降。
3. 在应用中，k值一般取一个比较小的值，通常采用交叉验证法来来选取最优的K值。

##### k折交叉验证：

###### 目的：

选出最为适合的模型超参数的取值，然后将超参数的值作用到模型的创建中。

###### 思想：

将样本的原始数据交叉的拆分出不同的训练集和验证集，使用交叉拆分出不同的训练集和验证集测分别试模型的精准度，然就求出的精准度的均值就是此次交叉验证的结果。将交叉验证作用到不同的超参数中，选取出精准度最高的超参数作为模型创建的超参数即可！

###### 实现思路：

1. 将数据集平均分割成K个等份
2. 使用1份数据作为测试数据，其余作为训练数据
3. 计算测试准确率
4. 使用不同的测试集，重复2、3步骤
5. 对准确率做平均，作为对未知数据预测准确率的估计![image-20200205181056332](F:\Python学习笔记\图片\image-20200205181056332.png)

###### API：

1. from sklearn.model_selection import cross_val_score
2. cross_val_score(estimator,X,y,cv):
   - estimator:模型对象
   - X,y:未拆分的特征和目标数据
   - cv：折数
     - 将原始的样本数据平均分割成cv值所对应的等分（n个等分），1/n作为测试集，n-1/n作为训练集
     - 对折次数：将原始数据进行n次指定比例的划分，从而选取n组不同的训练集和测试集

```
#将交叉验证作用到knn对鸢尾花的分类案例中
feature = datasets.load_iris()['data']
target = datasets.load_iris()['target']

knn = KNeighborsClassifier(n_neighbors=11)

cross_val_score(knn,feature,target,cv=5).mean()

KNN中的k值为5，不一定是最佳的选择，所以我们可以找出一个最佳的K
import matplotlib.pyplot as plt
#1.认为事先准备一组k值
k_values = [3,5,7,9,13,15,20,25,30]#准备好测试的超参数k的取值
#准备好的未拆分的样本数据
feature = datasets.load_iris()['data']
target = datasets.load_iris()['target']

score_list = []#存储每一个k值交叉验证后的均值

#以此将不同的k值带入到模型，然后使用交叉验证测试模型的精准度
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    #注意：在交叉验证中我们就不需要在手动调用fit对模型进行训练
    
    #获取了交叉验证对于k=？对应的测评分值
    score = cross_val_score(knn,feature,target,cv=5).mean()#先训练模型在做测评评分
    score_list.append(score)

plt.plot(k_values,score_list)  
plt.xlabel('k_value')
plt.ylabel('cross_mean_score')

最优的k值为6-7，所以我们就可以将k的最优值带入到模型对其进行训练
```

![image-20200205181334499](F:\Python学习笔记\图片\image-20200205181334499.png)

###### 使用KNN和logistic回归模型进行模型的比较和选择：

```
from sklearn.linear_model import LogisticRegression
knn = KNeighborsClassifier(n_neighbors=5)
print (cross_val_score(knn, train, test, cv=10).mean())
lr = LogisticRegression()
print(cross_val_score(lr,train,test,cv=10).mean())

结果：
0.9666666666666668
0.9533333333333334
所以我们选择knn作为回归模型
```

##### K-Fold&cross_val_score：

1. Scikit中指供了K-Fold的API

   - n-split就是折数
   - shuffle指是否对数据洗牌
   - random_state为随机种子,固定随机性

2. Scikit中提取带K-Fold接口的交叉验证接口sklearn.model_selection.cross_validate，但是该接口没有数据shuffle功能，所以一般结合Kfold一起使用。如果Train数据在分组前已经经过了shuffle处理，比如使用train_test_split分组，那就可以直接使用cross_val_score接口

   ```python
   import numpy as np
   from sklearn import datasets
   from sklearn.model_selection import KFold
   from sklearn.neighbors import KNeighborsClassifier
   from sklearn.model_selection import cross_val_score
   
   iris = datasets.load_iris()
   X, y = iris.data, iris.target
   
   knn = KNeighborsClassifier(n_neighbors=5)
   
   n_folds = 5
   kf = KFold(n_folds, shuffle=True, random_state=42)
   scores = cross_val_score(knn, X,y,cv = kf)
   
   scores.mean()
   
   
   结果为：0.9733333333333334
   ```

   